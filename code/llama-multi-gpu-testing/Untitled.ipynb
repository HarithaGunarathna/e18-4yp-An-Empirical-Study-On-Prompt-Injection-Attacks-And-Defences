{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c93f6ca-df6b-473d-8768-3db5d9ec6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from statistics import mean\n",
    "import torch, time, json\n",
    "\n",
    "prompts=[\n",
    "    \"The King is dead. Long live the Queen.\",\n",
    "    \"Once there were four children whose names were Peter, Susan, Edmund, and Lucy.\",\n",
    "    \"The story so far: in the beginning, the universe was created.\",\n",
    "    \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
    "    \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n",
    "    \"124 was spiteful. Full of Baby's venom.\",\n",
    "    \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect.\",\n",
    "    \"I write this sitting in the kitchen sink.\",\n",
    "    \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b54555-3ff5-409e-823f-d8b4b4056f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82acc92e059d44129d390e29047619c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected the presence of a `quantization_config` attribute in the model's configuration but you don't have the correct `bitsandbytes` version to support int8 serialization. Please install the latest version of `bitsandbytes` with  `pip install --upgrade bitsandbytes`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6804e8d98ac46e5a479e9e4fc31685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at TheBloke/Llama-2-7B-Chat-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.13.mlp.gate_proj.g_idx', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.21.mlp.up_proj.qweight', 'model.layers.0.mlp.gate_proj.qzeros', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.14.mlp.up_proj.g_idx', 'model.layers.11.self_attn.k_proj.g_idx', 'model.layers.18.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.16.self_attn.o_proj.g_idx', 'model.layers.24.mlp.down_proj.qweight', 'model.layers.4.mlp.up_proj.scales', 'model.layers.12.mlp.gate_proj.qweight', 'model.layers.14.self_attn.q_proj.scales', 'model.layers.3.mlp.gate_proj.scales', 'model.layers.12.mlp.gate_proj.g_idx', 'model.layers.17.mlp.up_proj.scales', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.14.self_attn.v_proj.scales', 'model.layers.19.self_attn.v_proj.scales', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.qzeros', 'model.layers.1.self_attn.q_proj.scales', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.10.self_attn.o_proj.qzeros', 'model.layers.19.self_attn.q_proj.qweight', 'model.layers.15.mlp.down_proj.qweight', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.qweight', 'model.layers.7.mlp.down_proj.qweight', 'model.layers.7.mlp.gate_proj.qweight', 'model.layers.22.self_attn.o_proj.g_idx', 'model.layers.14.self_attn.o_proj.scales', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.11.mlp.gate_proj.scales', 'model.layers.29.mlp.gate_proj.qzeros', 'model.layers.22.mlp.gate_proj.qweight', 'model.layers.1.mlp.up_proj.scales', 'model.layers.20.mlp.up_proj.bias', 'model.layers.2.mlp.up_proj.qweight', 'model.layers.20.self_attn.o_proj.qzeros', 'model.layers.22.mlp.down_proj.g_idx', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.26.mlp.down_proj.g_idx', 'model.layers.17.mlp.gate_proj.qzeros', 'model.layers.21.mlp.up_proj.g_idx', 'model.layers.24.mlp.up_proj.g_idx', 'model.layers.11.self_attn.o_proj.qzeros', 'model.layers.24.self_attn.q_proj.qweight', 'model.layers.24.mlp.gate_proj.g_idx', 'model.layers.5.mlp.gate_proj.g_idx', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.19.mlp.gate_proj.qzeros', 'model.layers.30.mlp.up_proj.qweight', 'model.layers.3.self_attn.k_proj.qweight', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.qweight', 'model.layers.23.self_attn.k_proj.qweight', 'model.layers.18.mlp.down_proj.qweight', 'model.layers.18.self_attn.q_proj.qzeros', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.g_idx', 'model.layers.5.self_attn.k_proj.scales', 'model.layers.30.mlp.down_proj.qweight', 'model.layers.16.self_attn.v_proj.qweight', 'model.layers.29.self_attn.q_proj.scales', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.9.self_attn.o_proj.qzeros', 'model.layers.1.self_attn.q_proj.qzeros', 'model.layers.16.self_attn.v_proj.g_idx', 'model.layers.30.self_attn.v_proj.scales', 'model.layers.11.mlp.up_proj.qweight', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.28.self_attn.o_proj.qweight', 'model.layers.1.mlp.gate_proj.g_idx', 'model.layers.30.mlp.up_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.g_idx', 'model.layers.5.self_attn.o_proj.g_idx', 'model.layers.20.mlp.down_proj.bias', 'model.layers.31.mlp.up_proj.qzeros', 'model.layers.16.mlp.down_proj.qzeros', 'model.layers.21.self_attn.k_proj.scales', 'model.layers.26.mlp.up_proj.qweight', 'model.layers.6.mlp.up_proj.qweight', 'model.layers.3.self_attn.q_proj.qzeros', 'model.layers.30.mlp.gate_proj.g_idx', 'model.layers.17.mlp.down_proj.qzeros', 'model.layers.14.mlp.gate_proj.qweight', 'model.layers.9.mlp.down_proj.qzeros', 'model.layers.9.mlp.gate_proj.g_idx', 'model.layers.6.mlp.gate_proj.qzeros', 'model.layers.20.mlp.down_proj.qzeros', 'model.layers.23.self_attn.q_proj.qzeros', 'model.layers.9.mlp.down_proj.qweight', 'model.layers.25.mlp.down_proj.scales', 'model.layers.29.mlp.up_proj.g_idx', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.16.self_attn.k_proj.qzeros', 'model.layers.31.self_attn.k_proj.g_idx', 'model.layers.6.mlp.down_proj.qzeros', 'model.layers.22.mlp.up_proj.scales', 'model.layers.31.self_attn.k_proj.scales', 'model.layers.31.self_attn.v_proj.g_idx', 'model.layers.16.self_attn.k_proj.qweight', 'model.layers.30.mlp.gate_proj.qweight', 'model.layers.4.self_attn.q_proj.qweight', 'model.layers.30.mlp.up_proj.scales', 'model.layers.14.self_attn.q_proj.qzeros', 'model.layers.17.mlp.down_proj.bias', 'model.layers.23.self_attn.v_proj.qweight', 'model.layers.19.mlp.down_proj.scales', 'model.layers.24.self_attn.o_proj.qzeros', 'model.layers.27.self_attn.k_proj.scales', 'model.layers.5.mlp.down_proj.g_idx', 'model.layers.5.self_attn.v_proj.qweight', 'model.layers.19.self_attn.v_proj.qzeros', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.g_idx', 'model.layers.11.self_attn.k_proj.qzeros', 'model.layers.17.mlp.gate_proj.qweight', 'model.layers.26.mlp.up_proj.scales', 'model.layers.31.self_attn.k_proj.qweight', 'model.layers.25.mlp.gate_proj.scales', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.4.self_attn.k_proj.qweight', 'model.layers.0.self_attn.k_proj.qzeros', 'model.layers.7.mlp.gate_proj.g_idx', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.qzeros', 'model.layers.18.self_attn.o_proj.qzeros', 'model.layers.21.self_attn.o_proj.g_idx', 'model.layers.5.mlp.gate_proj.scales', 'model.layers.26.self_attn.o_proj.g_idx', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.16.mlp.up_proj.qweight', 'model.layers.24.self_attn.o_proj.qweight', 'model.layers.30.mlp.up_proj.qzeros', 'model.layers.19.self_attn.v_proj.qweight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.qweight', 'model.layers.18.self_attn.q_proj.scales', 'model.layers.6.self_attn.q_proj.g_idx', 'model.layers.4.self_attn.o_proj.qzeros', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.qzeros', 'model.layers.17.self_attn.v_proj.g_idx', 'model.layers.22.self_attn.v_proj.qweight', 'model.layers.15.self_attn.q_proj.g_idx', 'model.layers.16.mlp.down_proj.qweight', 'model.layers.9.self_attn.k_proj.qweight', 'model.layers.30.self_attn.o_proj.g_idx', 'model.layers.18.self_attn.k_proj.scales', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.qweight', 'model.layers.25.mlp.up_proj.g_idx', 'model.layers.2.self_attn.v_proj.qweight', 'model.layers.6.self_attn.o_proj.qzeros', 'model.layers.20.self_attn.q_proj.g_idx', 'model.layers.18.self_attn.v_proj.g_idx', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.scales', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.22.mlp.gate_proj.g_idx', 'model.layers.17.self_attn.q_proj.g_idx', 'model.layers.21.self_attn.v_proj.g_idx', 'model.layers.18.mlp.up_proj.g_idx', 'model.layers.12.mlp.down_proj.qzeros', 'model.layers.29.self_attn.k_proj.qweight', 'model.layers.10.mlp.up_proj.g_idx', 'model.layers.26.mlp.down_proj.scales', 'model.layers.24.mlp.gate_proj.qzeros', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.11.mlp.gate_proj.g_idx', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.10.self_attn.o_proj.scales', 'model.layers.27.mlp.down_proj.g_idx', 'model.layers.7.self_attn.o_proj.g_idx', 'model.layers.24.self_attn.v_proj.qzeros', 'model.layers.27.self_attn.q_proj.qweight', 'model.layers.6.mlp.gate_proj.g_idx', 'model.layers.28.mlp.down_proj.scales', 'model.layers.21.mlp.gate_proj.scales', 'model.layers.13.self_attn.o_proj.qzeros', 'model.layers.24.self_attn.q_proj.qzeros', 'model.layers.31.mlp.down_proj.scales', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.qzeros', 'model.layers.5.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.qzeros', 'model.layers.8.mlp.down_proj.bias', 'model.layers.27.mlp.down_proj.qzeros', 'model.layers.21.self_attn.v_proj.qweight', 'model.layers.21.mlp.up_proj.qzeros', 'model.layers.30.self_attn.v_proj.g_idx', 'model.layers.30.mlp.up_proj.g_idx', 'model.layers.1.mlp.up_proj.qweight', 'model.layers.7.mlp.up_proj.qweight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.11.mlp.up_proj.qzeros', 'model.layers.12.mlp.gate_proj.scales', 'model.layers.15.self_attn.q_proj.qweight', 'model.layers.23.self_attn.o_proj.scales', 'model.layers.3.mlp.gate_proj.g_idx', 'model.layers.6.self_attn.v_proj.g_idx', 'model.layers.25.mlp.down_proj.qweight', 'model.layers.12.self_attn.v_proj.qzeros', 'model.layers.24.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.28.mlp.gate_proj.scales', 'model.layers.1.self_attn.o_proj.scales', 'model.layers.26.mlp.gate_proj.qzeros', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.9.mlp.gate_proj.scales', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.14.self_attn.o_proj.qzeros', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.20.self_attn.k_proj.scales', 'model.layers.1.self_attn.k_proj.scales', 'model.layers.10.self_attn.k_proj.g_idx', 'model.layers.1.mlp.down_proj.scales', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.scales', 'model.layers.19.mlp.gate_proj.qweight', 'model.layers.25.self_attn.q_proj.qzeros', 'model.layers.7.self_attn.q_proj.g_idx', 'model.layers.13.mlp.up_proj.qzeros', 'model.layers.8.self_attn.k_proj.qzeros', 'model.layers.2.self_attn.v_proj.qzeros', 'model.layers.5.self_attn.q_proj.scales', 'model.layers.1.mlp.up_proj.g_idx', 'model.layers.31.mlp.gate_proj.qzeros', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.qzeros', 'model.layers.23.self_attn.k_proj.qzeros', 'model.layers.18.self_attn.v_proj.qzeros', 'model.layers.18.self_attn.k_proj.qzeros', 'model.layers.8.mlp.up_proj.qzeros', 'model.layers.2.mlp.gate_proj.g_idx', 'model.layers.23.mlp.gate_proj.g_idx', 'model.layers.26.self_attn.q_proj.scales', 'model.layers.15.mlp.up_proj.scales', 'model.layers.8.self_attn.v_proj.scales', 'model.layers.13.self_attn.v_proj.g_idx', 'model.layers.15.self_attn.o_proj.qweight', 'model.layers.2.mlp.gate_proj.qzeros', 'model.layers.27.mlp.up_proj.qweight', 'model.layers.31.mlp.down_proj.qzeros', 'model.layers.23.self_attn.v_proj.scales', 'model.layers.29.mlp.gate_proj.g_idx', 'model.layers.11.mlp.up_proj.scales', 'model.layers.17.mlp.gate_proj.g_idx', 'model.layers.11.self_attn.q_proj.scales', 'model.layers.5.mlp.up_proj.g_idx', 'model.layers.4.mlp.gate_proj.g_idx', 'model.layers.12.mlp.down_proj.qweight', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.16.mlp.up_proj.g_idx', 'model.layers.26.self_attn.o_proj.qweight', 'model.layers.14.self_attn.v_proj.qweight', 'model.layers.16.mlp.gate_proj.qweight', 'model.layers.27.mlp.up_proj.scales', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.qweight', 'model.layers.11.mlp.up_proj.g_idx', 'model.layers.15.self_attn.q_proj.qzeros', 'model.layers.8.mlp.up_proj.g_idx', 'model.layers.27.self_attn.o_proj.qweight', 'model.layers.24.mlp.down_proj.scales', 'model.layers.29.self_attn.o_proj.scales', 'model.layers.0.self_attn.v_proj.scales', 'model.layers.15.mlp.up_proj.bias', 'model.layers.6.self_attn.v_proj.qweight', 'model.layers.4.mlp.up_proj.bias', 'model.layers.26.self_attn.v_proj.g_idx', 'model.layers.14.self_attn.v_proj.qzeros', 'model.layers.16.self_attn.o_proj.scales', 'model.layers.23.mlp.up_proj.g_idx', 'model.layers.31.mlp.up_proj.scales', 'model.layers.19.mlp.down_proj.qweight', 'model.layers.12.self_attn.v_proj.scales', 'model.layers.25.mlp.up_proj.scales', 'model.layers.6.self_attn.o_proj.scales', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.mlp.gate_proj.qweight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.9.self_attn.o_proj.scales', 'model.layers.16.self_attn.q_proj.scales', 'model.layers.23.self_attn.k_proj.scales', 'model.layers.17.self_attn.q_proj.scales', 'model.layers.20.mlp.down_proj.g_idx', 'model.layers.18.mlp.down_proj.scales', 'model.layers.0.mlp.up_proj.scales', 'model.layers.27.mlp.gate_proj.qzeros', 'model.layers.13.mlp.gate_proj.scales', 'model.layers.10.self_attn.q_proj.qzeros', 'model.layers.18.self_attn.k_proj.g_idx', 'model.layers.14.mlp.gate_proj.scales', 'model.layers.28.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.qweight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.g_idx', 'model.layers.18.self_attn.q_proj.g_idx', 'model.layers.16.mlp.gate_proj.qzeros', 'model.layers.1.mlp.down_proj.qzeros', 'model.layers.11.mlp.down_proj.qweight', 'model.layers.2.mlp.up_proj.g_idx', 'model.layers.28.self_attn.q_proj.scales', 'model.layers.24.self_attn.v_proj.scales', 'model.layers.0.mlp.gate_proj.qweight', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.qweight', 'model.layers.0.mlp.up_proj.qzeros', 'model.layers.17.self_attn.k_proj.scales', 'model.layers.5.mlp.down_proj.qweight', 'model.layers.5.mlp.gate_proj.qweight', 'model.layers.14.self_attn.k_proj.scales', 'model.layers.8.self_attn.q_proj.g_idx', 'model.layers.13.self_attn.o_proj.qweight', 'model.layers.24.self_attn.v_proj.qweight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.28.self_attn.o_proj.scales', 'model.layers.17.mlp.up_proj.g_idx', 'model.layers.0.mlp.down_proj.qweight', 'model.layers.11.self_attn.o_proj.g_idx', 'model.layers.28.mlp.down_proj.qzeros', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.qweight', 'model.layers.23.self_attn.o_proj.g_idx', 'model.layers.13.self_attn.o_proj.scales', 'model.layers.1.mlp.gate_proj.qweight', 'model.layers.13.self_attn.k_proj.g_idx', 'model.layers.13.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.g_idx', 'model.layers.29.mlp.up_proj.qzeros', 'model.layers.2.self_attn.k_proj.qzeros', 'model.layers.19.self_attn.o_proj.qzeros', 'model.layers.7.mlp.up_proj.scales', 'model.layers.21.self_attn.v_proj.qzeros', 'model.layers.26.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.qzeros', 'model.layers.14.self_attn.k_proj.qzeros', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.scales', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.g_idx', 'model.layers.21.mlp.down_proj.scales', 'model.layers.19.mlp.down_proj.g_idx', 'model.layers.13.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.g_idx', 'model.layers.26.mlp.down_proj.qzeros', 'model.layers.12.mlp.gate_proj.qzeros', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.mlp.gate_proj.qweight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.qweight', 'model.layers.6.self_attn.k_proj.scales', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.qweight', 'model.layers.10.mlp.gate_proj.qzeros', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.g_idx', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.18.mlp.gate_proj.g_idx', 'model.layers.31.self_attn.o_proj.scales', 'model.layers.7.mlp.gate_proj.qzeros', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.qzeros', 'model.layers.11.mlp.down_proj.scales', 'model.layers.8.self_attn.k_proj.scales', 'model.layers.2.self_attn.v_proj.scales', 'model.layers.20.self_attn.q_proj.scales', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.down_proj.g_idx', 'model.layers.16.mlp.down_proj.g_idx', 'model.layers.22.self_attn.v_proj.g_idx', 'model.layers.14.self_attn.v_proj.g_idx', 'model.layers.5.self_attn.k_proj.qweight', 'model.layers.12.self_attn.k_proj.qzeros', 'model.layers.19.self_attn.v_proj.g_idx', 'model.layers.8.mlp.down_proj.g_idx', 'model.layers.8.self_attn.k_proj.g_idx', 'model.layers.10.self_attn.v_proj.qzeros', 'model.layers.22.mlp.up_proj.qzeros', 'model.layers.4.self_attn.v_proj.g_idx', 'model.layers.25.self_attn.k_proj.qzeros', 'model.layers.0.self_attn.o_proj.qzeros', 'model.layers.10.mlp.down_proj.qzeros', 'model.layers.28.mlp.up_proj.scales', 'model.layers.24.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.14.mlp.down_proj.qweight', 'model.layers.28.mlp.gate_proj.qzeros', 'model.layers.30.self_attn.q_proj.qzeros', 'model.layers.12.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.qweight', 'model.layers.17.mlp.down_proj.scales', 'model.layers.20.mlp.down_proj.scales', 'model.layers.19.mlp.up_proj.scales', 'model.layers.2.self_attn.o_proj.qweight', 'model.layers.25.self_attn.q_proj.g_idx', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.10.mlp.gate_proj.qweight', 'model.layers.1.self_attn.v_proj.qzeros', 'model.layers.4.mlp.down_proj.bias', 'model.layers.2.self_attn.q_proj.g_idx', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.21.mlp.gate_proj.qzeros', 'model.layers.25.self_attn.o_proj.qweight', 'model.layers.26.self_attn.v_proj.qzeros', 'model.layers.28.self_attn.v_proj.qweight', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.24.mlp.gate_proj.scales', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.qweight', 'model.layers.6.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.g_idx', 'model.layers.18.self_attn.v_proj.scales', 'model.layers.25.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.scales', 'model.layers.21.mlp.gate_proj.g_idx', 'model.layers.22.self_attn.q_proj.scales', 'model.layers.26.self_attn.o_proj.qzeros', 'model.layers.18.mlp.gate_proj.scales', 'model.layers.28.mlp.down_proj.qweight', 'model.layers.15.mlp.down_proj.g_idx', 'model.layers.29.self_attn.v_proj.scales', 'model.layers.17.mlp.gate_proj.scales', 'model.layers.17.mlp.up_proj.qzeros', 'model.layers.6.mlp.down_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.9.mlp.down_proj.scales', 'model.layers.2.mlp.up_proj.bias', 'model.layers.23.mlp.gate_proj.scales', 'model.layers.15.self_attn.v_proj.scales', 'model.layers.26.self_attn.q_proj.qweight', 'model.layers.5.self_attn.q_proj.qweight', 'model.layers.6.mlp.up_proj.g_idx', 'model.layers.20.self_attn.o_proj.g_idx', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.16.mlp.up_proj.qzeros', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.scales', 'model.layers.31.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.g_idx', 'model.layers.17.self_attn.o_proj.scales', 'model.layers.29.self_attn.v_proj.qzeros', 'model.layers.15.mlp.gate_proj.g_idx', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.g_idx', 'model.layers.5.mlp.gate_proj.qzeros', 'model.layers.9.self_attn.v_proj.scales', 'model.layers.21.self_attn.q_proj.scales', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.23.mlp.up_proj.qzeros', 'model.layers.6.mlp.down_proj.scales', 'model.layers.22.mlp.up_proj.bias', 'model.layers.24.self_attn.o_proj.g_idx', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.22.self_attn.k_proj.scales', 'model.layers.26.self_attn.v_proj.scales', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.g_idx', 'model.layers.21.self_attn.o_proj.qzeros', 'model.layers.19.self_attn.o_proj.qweight', 'model.layers.25.self_attn.v_proj.qweight', 'model.layers.1.self_attn.k_proj.g_idx', 'model.layers.2.self_attn.o_proj.g_idx', 'model.layers.13.mlp.down_proj.g_idx', 'model.layers.7.self_attn.k_proj.scales', 'model.layers.19.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.qweight', 'model.layers.20.self_attn.k_proj.qzeros', 'model.layers.4.self_attn.v_proj.qweight', 'model.layers.12.mlp.down_proj.g_idx', 'model.layers.10.mlp.up_proj.bias', 'model.layers.22.self_attn.q_proj.qweight', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.24.mlp.down_proj.qzeros', 'model.layers.14.mlp.down_proj.bias', 'model.layers.20.self_attn.v_proj.qweight', 'model.layers.30.self_attn.o_proj.scales', 'model.layers.31.self_attn.o_proj.qzeros', 'model.layers.3.mlp.down_proj.g_idx', 'model.layers.12.mlp.up_proj.qzeros', 'model.layers.0.self_attn.o_proj.g_idx', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.qweight', 'model.layers.0.self_attn.v_proj.qzeros', 'model.layers.23.self_attn.v_proj.qzeros', 'model.layers.17.self_attn.k_proj.qzeros', 'model.layers.6.mlp.down_proj.qweight', 'model.layers.21.self_attn.k_proj.g_idx', 'model.layers.12.mlp.up_proj.scales', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.g_idx', 'model.layers.26.mlp.down_proj.bias', 'model.layers.4.self_attn.q_proj.g_idx', 'model.layers.1.mlp.down_proj.qweight', 'model.layers.15.self_attn.k_proj.qweight', 'model.layers.4.self_attn.q_proj.scales', 'model.layers.29.self_attn.o_proj.g_idx', 'model.layers.0.self_attn.v_proj.qweight', 'model.layers.3.self_attn.q_proj.g_idx', 'model.layers.13.self_attn.q_proj.g_idx', 'model.layers.26.self_attn.k_proj.scales', 'model.layers.17.self_attn.q_proj.qweight', 'model.layers.29.self_attn.q_proj.qzeros', 'model.layers.26.mlp.gate_proj.qweight', 'model.layers.6.self_attn.o_proj.g_idx', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.qweight', 'model.layers.15.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.qzeros', 'model.layers.28.mlp.down_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.g_idx', 'model.layers.19.self_attn.q_proj.qzeros', 'model.layers.30.self_attn.k_proj.scales', 'model.layers.21.self_attn.v_proj.scales', 'model.layers.15.self_attn.o_proj.scales', 'model.layers.13.self_attn.v_proj.qweight', 'model.layers.25.mlp.up_proj.qzeros', 'model.layers.27.self_attn.q_proj.g_idx', 'model.layers.29.self_attn.o_proj.qzeros', 'model.layers.4.mlp.down_proj.qzeros', 'model.layers.31.mlp.up_proj.g_idx', 'model.layers.20.mlp.gate_proj.qweight', 'model.layers.4.self_attn.o_proj.g_idx', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.qzeros', 'model.layers.21.self_attn.k_proj.qweight', 'model.layers.13.mlp.gate_proj.qweight', 'model.layers.10.self_attn.v_proj.scales', 'model.layers.16.mlp.down_proj.scales', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.qzeros', 'model.layers.1.mlp.gate_proj.qzeros', 'model.layers.26.mlp.gate_proj.g_idx', 'model.layers.24.self_attn.o_proj.scales', 'model.layers.15.mlp.down_proj.scales', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.qzeros', 'model.layers.29.self_attn.k_proj.scales', 'model.layers.5.self_attn.o_proj.qweight', 'model.layers.8.self_attn.o_proj.scales', 'model.layers.9.self_attn.k_proj.scales', 'model.layers.23.mlp.gate_proj.qweight', 'model.layers.6.self_attn.k_proj.qweight', 'model.layers.17.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.v_proj.g_idx', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.mlp.down_proj.qzeros', 'model.layers.7.mlp.gate_proj.scales', 'model.layers.28.mlp.down_proj.g_idx', 'model.layers.8.mlp.down_proj.scales', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.scales', 'model.layers.14.mlp.gate_proj.qzeros', 'model.layers.29.self_attn.o_proj.qweight', 'model.layers.14.self_attn.o_proj.g_idx', 'model.layers.7.self_attn.k_proj.qweight', 'model.layers.16.mlp.up_proj.scales', 'model.layers.1.mlp.gate_proj.scales', 'model.layers.21.mlp.down_proj.bias', 'model.layers.27.self_attn.v_proj.qzeros', 'model.layers.7.self_attn.o_proj.qzeros', 'model.layers.27.self_attn.v_proj.scales', 'model.layers.22.self_attn.k_proj.qzeros', 'model.layers.20.mlp.gate_proj.g_idx', 'model.layers.6.mlp.gate_proj.scales', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.25.mlp.gate_proj.qzeros', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.2.mlp.gate_proj.scales', 'model.layers.21.mlp.down_proj.g_idx', 'model.layers.18.mlp.up_proj.qzeros', 'model.layers.28.self_attn.q_proj.g_idx', 'model.layers.31.self_attn.q_proj.scales', 'model.layers.16.mlp.down_proj.bias', 'model.layers.0.mlp.up_proj.g_idx', 'model.layers.20.self_attn.v_proj.g_idx', 'model.layers.5.self_attn.q_proj.g_idx', 'model.layers.19.mlp.down_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.qzeros', 'model.layers.0.mlp.up_proj.qweight', 'model.layers.25.self_attn.q_proj.qweight', 'model.layers.28.self_attn.v_proj.g_idx', 'model.layers.29.mlp.up_proj.qweight', 'model.layers.17.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.g_idx', 'model.layers.20.mlp.up_proj.qzeros', 'model.layers.5.self_attn.v_proj.g_idx', 'model.layers.20.self_attn.q_proj.qweight', 'model.layers.12.mlp.down_proj.bias', 'model.layers.19.mlp.up_proj.qzeros', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.10.self_attn.v_proj.g_idx', 'model.layers.12.mlp.up_proj.qweight', 'model.layers.5.self_attn.k_proj.qzeros', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.8.self_attn.v_proj.g_idx', 'model.layers.6.self_attn.k_proj.g_idx', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.g_idx', 'model.layers.1.self_attn.q_proj.g_idx', 'model.layers.22.mlp.down_proj.qweight', 'model.layers.5.self_attn.o_proj.scales', 'model.layers.23.self_attn.q_proj.scales', 'model.layers.31.self_attn.o_proj.g_idx', 'model.layers.10.mlp.down_proj.qweight', 'model.layers.11.mlp.down_proj.qzeros', 'model.layers.31.self_attn.v_proj.scales', 'model.layers.13.self_attn.q_proj.scales', 'model.layers.3.self_attn.o_proj.qzeros', 'model.layers.5.self_attn.o_proj.qzeros', 'model.layers.14.mlp.up_proj.qzeros', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.scales', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.8.mlp.gate_proj.g_idx', 'model.layers.8.self_attn.q_proj.scales', 'model.layers.13.self_attn.v_proj.scales', 'model.layers.14.self_attn.k_proj.g_idx', 'model.layers.23.mlp.up_proj.scales', 'model.layers.18.mlp.down_proj.g_idx', 'model.layers.6.self_attn.o_proj.qweight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.qzeros', 'model.layers.27.mlp.up_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.7.self_attn.o_proj.scales', 'model.layers.17.self_attn.o_proj.qzeros', 'model.layers.12.self_attn.k_proj.g_idx', 'model.layers.11.self_attn.o_proj.scales', 'model.layers.28.mlp.gate_proj.qweight', 'model.layers.3.mlp.down_proj.scales', 'model.layers.0.mlp.up_proj.bias', 'model.layers.18.mlp.gate_proj.qweight', 'model.layers.9.mlp.gate_proj.qweight', 'model.layers.4.mlp.down_proj.qweight', 'model.layers.6.self_attn.q_proj.scales', 'model.layers.2.mlp.up_proj.scales', 'model.layers.19.self_attn.o_proj.g_idx', 'model.layers.24.mlp.gate_proj.qweight', 'model.layers.28.mlp.gate_proj.g_idx', 'model.layers.31.mlp.gate_proj.g_idx', 'model.layers.15.mlp.up_proj.g_idx', 'model.layers.3.mlp.up_proj.scales', 'model.layers.8.self_attn.q_proj.qweight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.22.self_attn.q_proj.qzeros', 'model.layers.25.mlp.down_proj.qzeros', 'model.layers.26.self_attn.k_proj.qzeros', 'model.layers.8.mlp.gate_proj.qzeros', 'model.layers.10.self_attn.k_proj.qweight', 'model.layers.30.self_attn.k_proj.qzeros', 'model.layers.1.self_attn.o_proj.g_idx', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.scales', 'model.layers.26.self_attn.k_proj.qweight', 'model.layers.30.self_attn.o_proj.qweight', 'model.layers.8.self_attn.q_proj.qzeros', 'model.layers.0.self_attn.k_proj.g_idx', 'model.layers.15.mlp.up_proj.qweight', 'model.layers.25.mlp.down_proj.g_idx', 'model.layers.6.self_attn.v_proj.qzeros', 'model.layers.28.self_attn.k_proj.qzeros', 'model.layers.16.self_attn.q_proj.g_idx', 'model.layers.16.self_attn.o_proj.qweight', 'model.layers.20.self_attn.k_proj.qweight', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.15.self_attn.o_proj.qzeros', 'model.layers.2.self_attn.k_proj.qweight', 'model.layers.20.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.k_proj.scales', 'model.layers.0.mlp.gate_proj.g_idx', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.11.self_attn.v_proj.scales', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.29.self_attn.k_proj.qzeros', 'model.layers.24.self_attn.v_proj.g_idx', 'model.layers.10.mlp.gate_proj.scales', 'model.layers.14.mlp.up_proj.scales', 'model.layers.26.self_attn.q_proj.g_idx', 'model.layers.25.self_attn.v_proj.scales', 'model.layers.0.mlp.down_proj.qzeros', 'model.layers.15.self_attn.v_proj.qweight', 'model.layers.15.self_attn.q_proj.scales', 'model.layers.30.mlp.down_proj.g_idx', 'model.layers.13.self_attn.o_proj.g_idx', 'model.layers.14.mlp.down_proj.qzeros', 'model.layers.10.mlp.gate_proj.g_idx', 'model.layers.7.self_attn.v_proj.scales', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.qzeros', 'model.layers.23.mlp.down_proj.qweight', 'model.layers.12.self_attn.k_proj.qweight', 'model.layers.19.self_attn.q_proj.scales', 'model.layers.19.mlp.up_proj.g_idx', 'model.layers.2.self_attn.q_proj.qweight', 'model.layers.8.self_attn.v_proj.qweight', 'model.layers.18.mlp.up_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.4.mlp.down_proj.scales', 'model.layers.30.mlp.gate_proj.qzeros', 'model.layers.0.mlp.gate_proj.scales', 'model.layers.24.self_attn.q_proj.scales', 'model.layers.25.self_attn.v_proj.g_idx', 'model.layers.9.mlp.up_proj.qzeros', 'model.layers.13.mlp.down_proj.qweight', 'model.layers.14.self_attn.k_proj.qweight', 'model.layers.12.self_attn.v_proj.qweight', 'model.layers.11.self_attn.q_proj.qzeros', 'model.layers.31.self_attn.v_proj.qzeros', 'model.layers.4.mlp.gate_proj.qzeros', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.0.self_attn.k_proj.qweight', 'model.layers.4.self_attn.v_proj.scales', 'model.layers.12.self_attn.q_proj.g_idx', 'model.layers.27.mlp.down_proj.qweight', 'model.layers.9.mlp.down_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.6.mlp.down_proj.g_idx', 'model.layers.27.self_attn.k_proj.qzeros', 'model.layers.17.mlp.down_proj.g_idx', 'model.layers.23.mlp.down_proj.bias', 'model.layers.24.mlp.up_proj.qzeros', 'model.layers.3.self_attn.o_proj.qweight', 'model.layers.21.mlp.down_proj.qweight', 'model.layers.8.mlp.up_proj.qweight', 'model.layers.25.self_attn.k_proj.g_idx', 'model.layers.21.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.qweight', 'model.layers.29.mlp.down_proj.g_idx', 'model.layers.26.mlp.gate_proj.scales', 'model.layers.10.self_attn.q_proj.g_idx', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.27.self_attn.o_proj.qzeros', 'model.layers.12.self_attn.k_proj.scales', 'model.layers.24.mlp.down_proj.g_idx', 'model.layers.18.self_attn.o_proj.qweight', 'model.layers.25.mlp.up_proj.qweight', 'model.layers.28.mlp.up_proj.qweight', 'model.layers.27.mlp.up_proj.qzeros', 'model.layers.20.mlp.gate_proj.qzeros', 'model.layers.25.self_attn.o_proj.g_idx', 'model.layers.13.mlp.up_proj.scales', 'model.layers.7.mlp.up_proj.qzeros', 'model.layers.27.mlp.gate_proj.qweight', 'model.layers.12.self_attn.o_proj.qzeros', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.13.mlp.up_proj.g_idx', 'model.layers.27.self_attn.q_proj.qzeros', 'model.layers.4.mlp.up_proj.qzeros', 'model.layers.23.self_attn.q_proj.qweight', 'model.layers.4.self_attn.k_proj.qzeros', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.qzeros', 'model.layers.9.self_attn.q_proj.g_idx', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.23.self_attn.o_proj.qzeros', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.0.mlp.down_proj.scales', 'model.layers.23.mlp.down_proj.g_idx', 'model.layers.23.mlp.down_proj.scales', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.31.self_attn.o_proj.qweight', 'model.layers.30.mlp.down_proj.scales', 'model.layers.3.self_attn.v_proj.qzeros', 'model.layers.10.self_attn.q_proj.qweight', 'model.layers.15.self_attn.k_proj.g_idx', 'model.layers.27.mlp.gate_proj.g_idx', 'model.layers.7.mlp.up_proj.g_idx', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.scales', 'model.layers.20.self_attn.v_proj.qzeros', 'model.layers.7.self_attn.v_proj.g_idx', 'model.layers.15.mlp.up_proj.qzeros', 'model.layers.12.self_attn.o_proj.scales', 'model.layers.30.mlp.gate_proj.scales', 'model.layers.18.self_attn.q_proj.qweight', 'model.layers.27.self_attn.v_proj.g_idx', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.30.self_attn.v_proj.qweight', 'model.layers.3.mlp.gate_proj.qzeros', 'model.layers.4.mlp.down_proj.g_idx', 'model.layers.18.mlp.up_proj.qweight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.scales', 'model.layers.18.self_attn.v_proj.qweight', 'model.layers.29.mlp.up_proj.scales', 'model.layers.3.mlp.gate_proj.qweight', 'model.layers.26.self_attn.q_proj.qzeros', 'model.layers.22.self_attn.q_proj.g_idx', 'model.layers.29.self_attn.k_proj.g_idx', 'model.layers.6.self_attn.k_proj.qzeros', 'model.layers.19.self_attn.k_proj.qweight', 'model.layers.23.mlp.down_proj.qzeros', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.31.mlp.gate_proj.qweight', 'model.layers.8.self_attn.v_proj.qzeros', 'model.layers.20.self_attn.o_proj.scales', 'model.layers.22.mlp.gate_proj.qzeros', 'model.layers.20.mlp.up_proj.scales', 'model.layers.21.mlp.down_proj.qzeros', 'model.layers.28.self_attn.o_proj.g_idx', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.qzeros', 'model.layers.21.self_attn.q_proj.g_idx', 'model.layers.7.mlp.down_proj.bias', 'model.layers.17.self_attn.o_proj.qweight', 'model.layers.14.self_attn.q_proj.qweight', 'model.layers.30.self_attn.q_proj.qweight', 'model.layers.10.self_attn.q_proj.scales', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.21.self_attn.k_proj.qzeros', 'model.layers.8.self_attn.o_proj.qweight', 'model.layers.30.mlp.down_proj.bias', 'model.layers.31.mlp.up_proj.qweight', 'model.layers.29.self_attn.q_proj.qweight', 'model.layers.27.self_attn.k_proj.g_idx', 'model.layers.30.self_attn.k_proj.qweight', 'model.layers.14.mlp.down_proj.g_idx', 'model.layers.1.self_attn.o_proj.qzeros', 'model.layers.10.mlp.up_proj.qzeros', 'model.layers.11.self_attn.o_proj.qweight', 'model.layers.5.mlp.up_proj.qzeros', 'model.layers.9.self_attn.o_proj.g_idx', 'model.layers.19.self_attn.k_proj.qzeros', 'model.layers.31.self_attn.q_proj.qzeros', 'model.layers.7.self_attn.q_proj.qweight', 'model.layers.27.mlp.up_proj.g_idx', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.qweight', 'model.layers.8.mlp.gate_proj.scales', 'model.layers.22.self_attn.o_proj.qweight', 'model.layers.3.self_attn.q_proj.qweight', 'model.layers.3.mlp.down_proj.qweight', 'model.layers.11.self_attn.v_proj.qzeros', 'model.layers.13.self_attn.q_proj.qzeros', 'model.layers.19.mlp.gate_proj.g_idx', 'model.layers.22.mlp.down_proj.qzeros', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.3.self_attn.o_proj.g_idx', 'model.layers.2.self_attn.o_proj.qzeros', 'model.layers.11.self_attn.v_proj.qweight', 'model.layers.11.self_attn.q_proj.g_idx', 'model.layers.0.self_attn.q_proj.g_idx', 'model.layers.11.mlp.gate_proj.qzeros', 'model.layers.19.mlp.gate_proj.scales', 'model.layers.6.mlp.gate_proj.qweight', 'model.layers.5.self_attn.v_proj.scales', 'model.layers.9.mlp.up_proj.qweight', 'model.layers.9.mlp.gate_proj.qzeros', 'model.layers.5.mlp.up_proj.bias', 'model.layers.15.mlp.gate_proj.qzeros', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.21.mlp.up_proj.scales', 'model.layers.7.self_attn.k_proj.qzeros', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.self_attn.o_proj.qweight', 'model.layers.12.self_attn.o_proj.qweight', 'model.layers.9.self_attn.v_proj.g_idx', 'model.layers.12.mlp.up_proj.g_idx', 'model.layers.25.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.scales', 'model.layers.24.self_attn.k_proj.qzeros', 'model.layers.23.mlp.gate_proj.qzeros', 'model.layers.13.mlp.up_proj.qweight', 'model.layers.26.mlp.up_proj.qzeros', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.29.self_attn.v_proj.qweight', 'model.layers.1.mlp.up_proj.qzeros', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.25.self_attn.k_proj.qweight', 'model.layers.11.mlp.down_proj.g_idx', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.qweight', 'model.layers.3.self_attn.k_proj.qzeros', 'model.layers.2.self_attn.q_proj.qzeros', 'model.layers.16.mlp.gate_proj.scales', 'model.layers.15.mlp.gate_proj.qweight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.g_idx', 'model.layers.17.self_attn.v_proj.scales', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.g_idx', 'model.layers.30.self_attn.q_proj.scales', 'model.layers.22.self_attn.k_proj.qweight', 'model.layers.20.mlp.up_proj.g_idx', 'model.layers.24.mlp.up_proj.qweight', 'model.layers.7.self_attn.q_proj.scales', 'model.layers.13.self_attn.k_proj.scales', 'model.layers.11.mlp.up_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.scales', 'model.layers.0.self_attn.o_proj.scales', 'model.layers.25.mlp.gate_proj.g_idx', 'model.layers.26.mlp.up_proj.g_idx', 'model.layers.8.self_attn.k_proj.qweight', 'model.layers.17.self_attn.q_proj.qzeros', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.qzeros', 'model.layers.21.self_attn.q_proj.qweight', 'model.layers.0.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.qzeros', 'model.layers.18.mlp.up_proj.scales', 'model.layers.4.mlp.gate_proj.scales', 'model.layers.7.self_attn.v_proj.qweight', 'model.layers.26.self_attn.v_proj.qweight', 'model.layers.22.mlp.up_proj.g_idx', 'model.layers.27.mlp.down_proj.bias', 'model.layers.12.self_attn.v_proj.g_idx', 'model.layers.18.mlp.down_proj.qzeros', 'model.layers.24.self_attn.k_proj.qweight', 'model.layers.9.mlp.up_proj.scales', 'model.layers.8.self_attn.o_proj.qzeros', 'model.layers.4.mlp.gate_proj.qweight', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.20.mlp.up_proj.qweight', 'model.layers.3.mlp.up_proj.bias', 'model.layers.7.mlp.down_proj.qzeros', 'model.layers.29.mlp.down_proj.qweight', 'model.layers.6.mlp.up_proj.qzeros', 'model.layers.11.self_attn.q_proj.qweight', 'model.layers.10.self_attn.v_proj.qweight', 'model.layers.15.mlp.gate_proj.scales', 'model.layers.16.mlp.gate_proj.g_idx', 'model.layers.16.self_attn.k_proj.g_idx', 'model.layers.17.self_attn.v_proj.qzeros', 'model.layers.3.self_attn.k_proj.scales', 'model.layers.9.self_attn.q_proj.scales', 'model.layers.5.mlp.up_proj.qweight', 'model.layers.9.self_attn.v_proj.qzeros', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.8.mlp.gate_proj.qweight', 'model.layers.7.self_attn.v_proj.qzeros', 'model.layers.2.self_attn.v_proj.g_idx', 'model.layers.13.mlp.gate_proj.qzeros', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.self_attn.k_proj.g_idx', 'model.layers.31.self_attn.k_proj.qzeros', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.5.mlp.down_proj.scales', 'model.layers.9.mlp.up_proj.g_idx', 'model.layers.22.self_attn.o_proj.scales', 'model.layers.15.self_attn.v_proj.qzeros', 'model.layers.20.mlp.gate_proj.scales', 'model.layers.9.mlp.down_proj.g_idx', 'model.layers.28.mlp.up_proj.qzeros', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.21.mlp.gate_proj.qweight', 'model.layers.2.mlp.down_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.3.self_attn.o_proj.scales', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.qzeros', 'model.layers.10.mlp.up_proj.scales', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.g_idx', 'model.layers.9.self_attn.o_proj.qweight', 'model.layers.17.self_attn.v_proj.qweight', 'model.layers.10.mlp.down_proj.bias', 'model.layers.21.self_attn.o_proj.scales', 'model.layers.8.mlp.up_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.g_idx', 'model.layers.17.mlp.down_proj.qweight', 'model.layers.23.self_attn.o_proj.qweight', 'model.layers.28.mlp.up_proj.g_idx', 'model.layers.28.self_attn.v_proj.scales', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.self_attn.q_proj.g_idx', 'model.layers.16.self_attn.q_proj.qweight', 'model.layers.18.self_attn.o_proj.scales', 'model.layers.10.mlp.up_proj.qweight', 'model.layers.10.self_attn.o_proj.qweight', 'model.layers.16.self_attn.v_proj.scales', 'model.layers.20.self_attn.v_proj.scales', 'model.layers.31.self_attn.q_proj.g_idx', 'model.layers.9.self_attn.v_proj.qweight', 'model.layers.13.self_attn.k_proj.qweight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.25.mlp.gate_proj.qweight', 'model.layers.18.self_attn.k_proj.qweight', 'model.layers.4.mlp.up_proj.qweight', 'model.layers.19.mlp.down_proj.qzeros', 'model.layers.19.self_attn.k_proj.scales', 'model.layers.13.mlp.down_proj.bias', 'model.layers.22.self_attn.k_proj.g_idx', 'model.layers.27.mlp.gate_proj.scales', 'model.layers.27.self_attn.o_proj.g_idx', 'model.layers.23.mlp.up_proj.qweight', 'model.layers.16.self_attn.o_proj.qzeros', 'model.layers.28.self_attn.q_proj.qzeros', 'model.layers.4.self_attn.k_proj.g_idx', 'model.layers.6.self_attn.q_proj.qweight', 'model.layers.0.self_attn.k_proj.scales', 'model.layers.4.self_attn.o_proj.scales', 'model.layers.12.self_attn.q_proj.qweight', 'model.layers.8.mlp.down_proj.qweight', 'model.layers.0.self_attn.q_proj.qzeros', 'model.layers.8.mlp.down_proj.qzeros', 'model.layers.15.mlp.down_proj.qzeros', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.gate_proj.qweight', 'model.layers.2.mlp.up_proj.qzeros', 'model.layers.27.self_attn.o_proj.scales', 'model.layers.15.self_attn.k_proj.qzeros', 'model.layers.13.mlp.down_proj.scales', 'model.layers.24.mlp.up_proj.scales', 'model.layers.9.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.q_proj.g_idx', 'model.layers.14.mlp.gate_proj.g_idx', 'model.layers.23.self_attn.q_proj.g_idx', 'model.layers.30.mlp.down_proj.qzeros', 'model.layers.29.self_attn.v_proj.g_idx', 'model.layers.10.self_attn.k_proj.scales', 'model.layers.0.self_attn.q_proj.qweight', 'model.layers.9.self_attn.q_proj.qzeros', 'model.layers.23.self_attn.k_proj.g_idx', 'model.layers.8.mlp.up_proj.scales', 'model.layers.9.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.o_proj.scales', 'model.layers.22.self_attn.v_proj.scales', 'model.layers.30.self_attn.v_proj.qzeros', 'model.layers.3.self_attn.v_proj.scales', 'model.layers.29.mlp.down_proj.scales', 'model.layers.1.self_attn.q_proj.qweight', 'model.layers.31.self_attn.v_proj.qweight', 'model.layers.17.self_attn.k_proj.qweight', 'model.layers.7.mlp.up_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.qzeros', 'model.layers.0.mlp.down_proj.g_idx', 'model.layers.31.mlp.gate_proj.scales', 'model.layers.1.mlp.down_proj.g_idx', 'model.layers.1.self_attn.v_proj.qweight', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.qweight', 'model.layers.6.mlp.up_proj.scales', 'model.layers.28.self_attn.k_proj.scales', 'model.layers.12.self_attn.o_proj.g_idx', 'model.layers.11.self_attn.k_proj.scales', 'model.layers.25.self_attn.q_proj.scales', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.g_idx', 'model.layers.25.self_attn.o_proj.qzeros', 'model.layers.12.self_attn.q_proj.qzeros', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.g_idx', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.scales', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.15.self_attn.v_proj.g_idx', 'model.layers.6.self_attn.q_proj.qzeros', 'model.layers.22.mlp.gate_proj.scales', 'model.layers.22.mlp.up_proj.qweight', 'model.layers.1.self_attn.v_proj.scales', 'model.layers.21.self_attn.o_proj.qweight', 'model.layers.10.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.qzeros', 'model.layers.7.mlp.down_proj.scales', 'model.layers.9.self_attn.q_proj.qweight', 'model.layers.2.self_attn.o_proj.scales', 'model.layers.1.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.o_proj.qweight', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.3.mlp.up_proj.qweight', 'model.layers.29.mlp.gate_proj.scales', 'model.layers.8.self_attn.o_proj.g_idx', 'model.layers.27.mlp.down_proj.scales', 'model.layers.3.self_attn.q_proj.scales', 'model.layers.30.self_attn.k_proj.g_idx', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.scales', 'model.layers.5.mlp.up_proj.scales', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.scales', 'model.layers.2.mlp.down_proj.scales', 'model.layers.0.self_attn.q_proj.scales', 'model.layers.17.mlp.up_proj.qweight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb88c71c03f74d71bc24d7dca6c1588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load a base model and tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-7B-Chat-GPTQ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/transformers/modeling_utils.py:2846\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;66;03m# Dispatch model with hooks on all devices if necessary\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2846\u001b[0m     \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   2849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/accelerate/big_modeling.py:488\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    486\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/transformers/modeling_utils.py:1896\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1892\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1893\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1894\u001b[0m     )\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/e18-fyp-g6/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# load a base model and tokenizer\n",
    "model_path=\"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,    \n",
    "    device_map={\"\": accelerator.process_index},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf1307-8327-4f3c-8b9b-dc162defd8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    prompt_tokenized=tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output_tokenized = model.generate(prompt_tokenized.input_ids, max_new_tokens=100)[0]\n",
    "\n",
    "    # remove prompt from output \n",
    "    output_tokenized=output_tokenized[len(prompt_tokenized[\"input_ids\"][0]):]\n",
    "\n",
    "    # store outputs and number of tokens in result{}\n",
    "    print(tokenizer.decode(output_tokenized))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
